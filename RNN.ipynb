{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/shakespeare_proc_rnn.txt', 'r') as f:\n",
    "    source = f.read()\n",
    "\n",
    "char_seqs = [source[i-40:i+1] for i in range(40, len(source))]\n",
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "for i, char in enumerate(set(source)):\n",
    "    char_to_int[char] = i\n",
    "    int_to_char[i] = char\n",
    "    \n",
    "num_chars = len(char_to_int)\n",
    "encoded = np.array([[char_to_int[char] for char in seq] for seq in char_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "93786/93786 [==============================] - 80s 857us/step - loss: 2.3191\n",
      "Epoch 2/50\n",
      "93786/93786 [==============================] - 79s 848us/step - loss: 1.9219\n",
      "Epoch 3/50\n",
      "93786/93786 [==============================] - 77s 817us/step - loss: 1.7793\n",
      "Epoch 4/50\n",
      "93786/93786 [==============================] - 79s 841us/step - loss: 1.6902\n",
      "Epoch 5/50\n",
      "93786/93786 [==============================] - 78s 827us/step - loss: 1.6235\n",
      "Epoch 6/50\n",
      "93786/93786 [==============================] - 81s 869us/step - loss: 1.5690\n",
      "Epoch 7/50\n",
      "93786/93786 [==============================] - 81s 861us/step - loss: 1.5207\n",
      "Epoch 8/50\n",
      "93786/93786 [==============================] - 77s 824us/step - loss: 1.4769\n",
      "Epoch 9/50\n",
      "93786/93786 [==============================] - 83s 882us/step - loss: 1.4353\n",
      "Epoch 10/50\n",
      "93786/93786 [==============================] - 78s 833us/step - loss: 1.3973\n",
      "Epoch 11/50\n",
      "93786/93786 [==============================] - 79s 844us/step - loss: 1.3606\n",
      "Epoch 12/50\n",
      "93786/93786 [==============================] - 79s 840us/step - loss: 1.3266\n",
      "Epoch 13/50\n",
      "93786/93786 [==============================] - 77s 821us/step - loss: 1.2924\n",
      "Epoch 14/50\n",
      "93786/93786 [==============================] - 76s 807us/step - loss: 1.2594\n",
      "Epoch 15/50\n",
      "93786/93786 [==============================] - 76s 806us/step - loss: 1.2280\n",
      "Epoch 16/50\n",
      "93786/93786 [==============================] - 80s 852us/step - loss: 1.1982\n",
      "Epoch 17/50\n",
      "93786/93786 [==============================] - 78s 834us/step - loss: 1.1706\n",
      "Epoch 18/50\n",
      "93786/93786 [==============================] - 78s 830us/step - loss: 1.1436\n",
      "Epoch 19/50\n",
      "93786/93786 [==============================] - 78s 827us/step - loss: 1.1203\n",
      "Epoch 20/50\n",
      "93786/93786 [==============================] - 78s 834us/step - loss: 1.0942\n",
      "Epoch 21/50\n",
      "93786/93786 [==============================] - 92s 980us/step - loss: 1.0713\n",
      "Epoch 22/50\n",
      "93786/93786 [==============================] - 84s 900us/step - loss: 1.0524\n",
      "Epoch 23/50\n",
      "93786/93786 [==============================] - 78s 833us/step - loss: 1.0303\n",
      "Epoch 24/50\n",
      "93786/93786 [==============================] - 80s 852us/step - loss: 1.0132\n",
      "Epoch 25/50\n",
      "93786/93786 [==============================] - 114s 1ms/step - loss: 0.9969\n",
      "Epoch 26/50\n",
      "93786/93786 [==============================] - 89s 947us/step - loss: 0.9803\n",
      "Epoch 27/50\n",
      "93786/93786 [==============================] - 78s 833us/step - loss: 0.9657\n",
      "Epoch 28/50\n",
      "93786/93786 [==============================] - 76s 815us/step - loss: 0.9520\n",
      "Epoch 29/50\n",
      "93786/93786 [==============================] - 77s 824us/step - loss: 0.9400\n",
      "Epoch 30/50\n",
      "93786/93786 [==============================] - 78s 837us/step - loss: 0.9290\n",
      "Epoch 31/50\n",
      "93786/93786 [==============================] - 78s 837us/step - loss: 0.9141\n",
      "Epoch 32/50\n",
      "93786/93786 [==============================] - 105s 1ms/step - loss: 0.9057\n",
      "Epoch 33/50\n",
      "93786/93786 [==============================] - 81s 862us/step - loss: 0.8978\n",
      "Epoch 34/50\n",
      "93786/93786 [==============================] - 79s 839us/step - loss: 0.8870\n",
      "Epoch 35/50\n",
      "93786/93786 [==============================] - 79s 844us/step - loss: 0.8777\n",
      "Epoch 36/50\n",
      "93786/93786 [==============================] - 79s 838us/step - loss: 0.8758\n",
      "Epoch 37/50\n",
      "93786/93786 [==============================] - 79s 839us/step - loss: 0.8657\n",
      "Epoch 38/50\n",
      "93786/93786 [==============================] - 79s 840us/step - loss: 0.8577\n",
      "Epoch 39/50\n",
      "93786/93786 [==============================] - 80s 851us/step - loss: 0.8498\n",
      "Epoch 40/50\n",
      "93786/93786 [==============================] - 84s 901us/step - loss: 0.8484\n",
      "Epoch 41/50\n",
      "93786/93786 [==============================] - 106s 1ms/step - loss: 0.8421\n",
      "Epoch 42/50\n",
      "93786/93786 [==============================] - 88s 943us/step - loss: 0.8338\n",
      "Epoch 43/50\n",
      "93786/93786 [==============================] - 84s 894us/step - loss: 0.8296\n",
      "Epoch 44/50\n",
      "93786/93786 [==============================] - 77s 825us/step - loss: 0.8228\n",
      "Epoch 45/50\n",
      "93786/93786 [==============================] - 77s 824us/step - loss: 0.8187\n",
      "Epoch 46/50\n",
      "93786/93786 [==============================] - 78s 831us/step - loss: 0.8113\n",
      "Epoch 47/50\n",
      "93786/93786 [==============================] - 78s 830us/step - loss: 0.8065\n",
      "Epoch 48/50\n",
      "93786/93786 [==============================] - 80s 848us/step - loss: 0.8033\n",
      "Epoch 49/50\n",
      "93786/93786 [==============================] - 90s 963us/step - loss: 0.7960\n",
      "Epoch 50/50\n",
      "93786/93786 [==============================] - 110s 1ms/step - loss: 0.7916\n"
     ]
    }
   ],
   "source": [
    "X = np.array([to_categorical(c, num_classes=num_chars) for c in encoded[:,:-1]])\n",
    "y = to_categorical(encoded[:,-1], num_classes=num_chars)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(num_chars, activation='softmax'))\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "model.fit(X, y, epochs=50)\n",
    "model.save('rnn_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([to_categorical(c, num_classes=num_chars) for c in encoded[:,:-1]])\n",
    "y = to_categorical(encoded[:,-1], num_classes=num_chars)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(50, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(num_chars, activation='softmax'))\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "model.fit(X, y, epochs=50)\n",
    "model.save('deep_rnn_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "that in the breath their victurn true,\n",
      "and therefore with thou art poart to my sight,\n",
      "where thou wert thou wilt, thou art plea trust live,\n",
      "my self ink in love that to blest eyes.\n",
      "that they see aprieving to the world to conspaice.\n",
      "but that tongue that i have some in their show,\n",
      "they doy presate that thou shouldst in men's love,\n",
      "when i am self in summer's not cone,\n",
      "which thou art trust before hath my true,\n",
      "still thou feeding and heart doth can despaised,\n",
      "even in the will sweet self appear where.\n",
      "in every new lose posterity the tweet,\n",
      "the clocked it in me that thou shouldst ploved.\n",
      "i till the gra\n"
     ]
    }
   ],
   "source": [
    "model = load_model('rnn_model')\n",
    "\n",
    "temperature = 0.25\n",
    "\n",
    "text = 'shall i compare thee to a summer\\'s day?\\n'\n",
    "for i in range(600):\n",
    "    encoded = [char_to_int[char] for char in text]\n",
    "    encoded = pad_sequences([encoded], maxlen=40, truncating='pre')\n",
    "    encoded = to_categorical(encoded, num_classes=num_chars)\n",
    "    pred = model.predict(encoded, verbose=0)\n",
    "    pred = np.exp(np.log(pred[0]) / temperature)\n",
    "    pred /= (np.sum(pred) + .00001)\n",
    "    output = np.argmax(np.random.multinomial(1, pred, 1))\n",
    "    text += int_to_char[output]\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "then beauteous shave no wert thou thy soul,\n",
      "i see thing a shadetre that my and thine.\n",
      "that thou gaves, that thy sweet self too,\n",
      "in such a self-love and strangely, and thee,\n",
      "they do i none doth her sight, and dost show,\n",
      "sweet think on thines to his sun then marged,\n",
      "when in the wantard thou gild'st the worn,\n",
      "and think in me thou desire im received\n",
      "for their excelled that like a summer's stay\n",
      "a sonly new on that love's fingers winten\n",
      "bearing the times are not seem do the day\n",
      "steal thy heart that which thou thy self dost seeing,\n",
      "and thoughts to the table is never make shows,\n",
      "and \n"
     ]
    }
   ],
   "source": [
    "model = load_model('deep_rnn_model')\n",
    "\n",
    "temperature = 0.25\n",
    "\n",
    "text = 'shall i compare thee to a summer\\'s day?\\n'\n",
    "for i in range(582):\n",
    "    encoded = [char_to_int[char] for char in text]\n",
    "    encoded = pad_sequences([encoded], maxlen=40, truncating='pre')\n",
    "    encoded = to_categorical(encoded, num_classes=num_chars)\n",
    "    pred = model.predict(encoded, verbose=0)\n",
    "    pred = np.exp(np.log(pred[0]) / temperature)\n",
    "    pred /= (np.sum(pred) + .00001)\n",
    "    output = np.argmax(np.random.multinomial(1, pred, 1))\n",
    "    text += int_to_char[output]\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
