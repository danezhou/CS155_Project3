{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Lambda\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/shakespeare_proc_rnn.txt', 'r') as f:\n",
    "    source = f.read()\n",
    "\n",
    "num_chars = len(set(source))\n",
    "char_seqs = [source[i-40:i+1] for i in range(40, len(source))]\n",
    "char_to_int = {}\n",
    "for i, char in enumerate(set(source)):\n",
    "    char_to_int[char] = i\n",
    "encoded = np.array([[char_to_int[char] for char in seq] for seq in char_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "93786/93786 [==============================] - 80s 857us/step - loss: 2.3191\n",
      "Epoch 2/50\n",
      "93786/93786 [==============================] - 79s 848us/step - loss: 1.9219\n",
      "Epoch 3/50\n",
      "93786/93786 [==============================] - 77s 817us/step - loss: 1.7793\n",
      "Epoch 4/50\n",
      "93786/93786 [==============================] - 79s 841us/step - loss: 1.6902\n",
      "Epoch 5/50\n",
      "93786/93786 [==============================] - 78s 827us/step - loss: 1.6235\n",
      "Epoch 6/50\n",
      "93786/93786 [==============================] - 81s 869us/step - loss: 1.5690\n",
      "Epoch 7/50\n",
      "93786/93786 [==============================] - 81s 861us/step - loss: 1.5207\n",
      "Epoch 8/50\n",
      "93786/93786 [==============================] - 77s 824us/step - loss: 1.4769\n",
      "Epoch 9/50\n",
      "93786/93786 [==============================] - 83s 882us/step - loss: 1.4353\n",
      "Epoch 10/50\n",
      "93786/93786 [==============================] - 78s 833us/step - loss: 1.3973\n",
      "Epoch 11/50\n",
      "93786/93786 [==============================] - 79s 844us/step - loss: 1.3606\n",
      "Epoch 12/50\n",
      "93786/93786 [==============================] - 79s 840us/step - loss: 1.3266\n",
      "Epoch 13/50\n",
      "93786/93786 [==============================] - 77s 821us/step - loss: 1.2924\n",
      "Epoch 14/50\n",
      "93786/93786 [==============================] - 76s 807us/step - loss: 1.2594\n",
      "Epoch 15/50\n",
      "93786/93786 [==============================] - 76s 806us/step - loss: 1.2280\n",
      "Epoch 16/50\n",
      "93786/93786 [==============================] - 80s 852us/step - loss: 1.1982\n",
      "Epoch 17/50\n",
      "93786/93786 [==============================] - 78s 834us/step - loss: 1.1706\n",
      "Epoch 18/50\n",
      "93786/93786 [==============================] - 78s 830us/step - loss: 1.1436\n",
      "Epoch 19/50\n",
      "93786/93786 [==============================] - 78s 827us/step - loss: 1.1203\n",
      "Epoch 20/50\n",
      "93786/93786 [==============================] - 78s 834us/step - loss: 1.0942\n",
      "Epoch 21/50\n",
      "93786/93786 [==============================] - 92s 980us/step - loss: 1.0713\n",
      "Epoch 22/50\n",
      "93786/93786 [==============================] - 84s 900us/step - loss: 1.0524\n",
      "Epoch 23/50\n",
      "93786/93786 [==============================] - 78s 833us/step - loss: 1.0303\n",
      "Epoch 24/50\n",
      "93786/93786 [==============================] - 80s 852us/step - loss: 1.0132\n",
      "Epoch 25/50\n",
      "93786/93786 [==============================] - 114s 1ms/step - loss: 0.9969\n",
      "Epoch 26/50\n",
      "93786/93786 [==============================] - 89s 947us/step - loss: 0.9803\n",
      "Epoch 27/50\n",
      "93786/93786 [==============================] - 78s 833us/step - loss: 0.9657\n",
      "Epoch 28/50\n",
      "93786/93786 [==============================] - 76s 815us/step - loss: 0.9520\n",
      "Epoch 29/50\n",
      "93786/93786 [==============================] - 77s 824us/step - loss: 0.9400\n",
      "Epoch 30/50\n",
      "93786/93786 [==============================] - 78s 837us/step - loss: 0.9290\n",
      "Epoch 31/50\n",
      "93786/93786 [==============================] - 78s 837us/step - loss: 0.9141\n",
      "Epoch 32/50\n",
      "93786/93786 [==============================] - 105s 1ms/step - loss: 0.9057\n",
      "Epoch 33/50\n",
      "93786/93786 [==============================] - 81s 862us/step - loss: 0.8978\n",
      "Epoch 34/50\n",
      "93786/93786 [==============================] - 79s 839us/step - loss: 0.8870\n",
      "Epoch 35/50\n",
      "93786/93786 [==============================] - 79s 844us/step - loss: 0.8777\n",
      "Epoch 36/50\n",
      "93786/93786 [==============================] - 79s 838us/step - loss: 0.8758\n",
      "Epoch 37/50\n",
      "93786/93786 [==============================] - 79s 839us/step - loss: 0.8657\n",
      "Epoch 38/50\n",
      "93786/93786 [==============================] - 79s 840us/step - loss: 0.8577\n",
      "Epoch 39/50\n",
      "93786/93786 [==============================] - 80s 851us/step - loss: 0.8498\n",
      "Epoch 40/50\n",
      "93786/93786 [==============================] - 84s 901us/step - loss: 0.8484\n",
      "Epoch 41/50\n",
      "93786/93786 [==============================] - 106s 1ms/step - loss: 0.8421\n",
      "Epoch 42/50\n",
      "93786/93786 [==============================] - 88s 943us/step - loss: 0.8338\n",
      "Epoch 43/50\n",
      "93786/93786 [==============================] - 84s 894us/step - loss: 0.8296\n",
      "Epoch 44/50\n",
      "93786/93786 [==============================] - 77s 825us/step - loss: 0.8228\n",
      "Epoch 45/50\n",
      "93786/93786 [==============================] - 77s 824us/step - loss: 0.8187\n",
      "Epoch 46/50\n",
      "93786/93786 [==============================] - 78s 831us/step - loss: 0.8113\n",
      "Epoch 47/50\n",
      "93786/93786 [==============================] - 78s 830us/step - loss: 0.8065\n",
      "Epoch 48/50\n",
      "93786/93786 [==============================] - 80s 848us/step - loss: 0.8033\n",
      "Epoch 49/50\n",
      "93786/93786 [==============================] - 90s 963us/step - loss: 0.7960\n",
      "Epoch 50/50\n",
      "93786/93786 [==============================] - 110s 1ms/step - loss: 0.7916\n"
     ]
    }
   ],
   "source": [
    "X, y = encoded[:,:-1], encoded[:,-1]\n",
    "X = np.array([to_categorical(x, num_classes=num_chars) for x in X])\n",
    "y = to_categorical(y, num_classes=num_chars)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(num_chars, activation='softmax'))\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "model.fit(X, y, epochs=50)\n",
    "model.save('rnn_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "what this in their fair lend fair, cowers confound.\n",
      "wherein thou wilt puse too, this in their skill,\n",
      "to rides of seesured that her through my state,\n",
      "and thou shoucds of thy hume, look doth gone,\n",
      "savanieved do the could with the rearly year!\n",
      "what offards do hours, or thy should thou grew?\n",
      "o my love may state in summer's new earth,\n",
      "far inmort noter, shall time thou wilt look, bein are metreace,\n",
      "whereother-fair o'erity that your self is cride.\n",
      "he piab a wond appear in race to reason,\n",
      "presaded lipser thus thou not sore can sit,\n",
      "thou condercess your his minure with their sorrow,\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "model = load_model('rnn_model')\n",
    "\n",
    "temperature = 0.75\n",
    "\n",
    "text = 'shall i compare thee to a summer\\'s day?\\n'\n",
    "for i in range(582):\n",
    "    encoded = [char_to_int[char] for char in text]\n",
    "    encoded = pad_sequences([encoded], maxlen=40, truncating='pre')\n",
    "    encoded = to_categorical(encoded, num_classes=len(char_to_int))\n",
    "    preds = model.predict(encoded, verbose=0)\n",
    "    pred = np.log(preds[0]) / temperature\n",
    "    exp_pred = np.exp(pred)\n",
    "    exp_pred /= (np.sum(exp_pred) + .00001)\n",
    "    out_index = np.argmax(np.random.multinomial(1, exp_pred, 1))\n",
    "    out_char = ''\n",
    "    for char, index in char_to_int.items():\n",
    "        if index == out_index:\n",
    "            out_char = char\n",
    "            break\n",
    "    text += char\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
